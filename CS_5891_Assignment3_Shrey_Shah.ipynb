{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Loading the MNIST dataset and performing the required preprocessing on the data. Reshaping the training data and testing data such that all the images are flattened into column and each column of the dataset X represents one training example. Additionally, normalizing the training and testing data by dividing each pixel value in the dataset by 255.  Reshaping the training and testing label to a row vector. The last digit of my student ID = 0, thus modifying the labels of the training and testing data to contain value 1 for an image containing the number 0 and a value of 0 otherwise. Additionally, printing the shapes of the training and testing data and labels to confirm correct preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set contains:  60000\n",
      "test set:  10000\n",
      "Shape of the training data (60000, 28, 28)\n",
      "shape of training label (60000,)\n",
      "(784, 60000)\n",
      "(784, 10000)\n",
      "(1, 60000)\n",
      "(1, 10000)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print('The training set contains: ', len(x_train))\n",
    "print('test set: ', len(x_test))\n",
    "print('Shape of the training data',x_train.shape)\n",
    "print('shape of training label',y_train.shape)\n",
    "# Performing the preprocessing steps on the data set\n",
    "# Reshaping the training and test set such that each column represents an image\n",
    "# Reshaping the training and test labels into row vectors\n",
    "x_train = np.reshape(x_train, (60000, -1))\n",
    "x_train = x_train.T\n",
    "y_train = np.reshape(y_train, (1, -1))\n",
    "x_test = np.reshape(x_test, (10000, -1))\n",
    "x_test = x_test.T\n",
    "y_test = np.reshape(y_test, (1, -1))\n",
    "# Normalizing the data in training and test sets\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "# Updating the labels in the training and test set\n",
    "# Last digit of my student id = 0\n",
    "# Thus, the label of an image of number 0 should be 1 and 0 otherwise\n",
    "# This is achieved by creating masks for the training and testing labels such that the masks have True for indices\n",
    "# which correspond to an image of number 0\n",
    "id_mask_train = y_train == 0\n",
    "id_mask_test = y_test == 0\n",
    "y_train[id_mask_train] = 1\n",
    "y_train[np.logical_not(id_mask_train)] = 0\n",
    "y_test[id_mask_test] = 1\n",
    "y_test[np.logical_not(id_mask_test)] = 0\n",
    "# Checking the shape of the training and test sets\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions and their Gradients\n",
    "Functions for implementing the sigmoid activation, tanh activation and ReLU activation functions. The function act_gradient computes and returns the gradeint of the activation function (provided as parameter). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_sigmoid(z):\n",
    "    A = 1/(1+np.exp(-z))\n",
    "    return A\n",
    "\n",
    "\n",
    "def act_tanh(z):\n",
    "    A = np.tanh(z)\n",
    "    return A\n",
    "\n",
    "\n",
    "def act_relu(z):\n",
    "    A = np.maximum(0, z)\n",
    "    return A\n",
    "\n",
    "\n",
    "def act_gradient(A, act = 'ReLU'):\n",
    "    dg = np.zeros_like(A)\n",
    "    if act == 'ReLU':\n",
    "        dg[A>0] = 1\n",
    "    elif act == 'sigmoid':\n",
    "        dg = A*(1 - A)\n",
    "    elif act == 'tanh':\n",
    "        dg = 1 - np.square(A)\n",
    "    \n",
    "    return dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Class\n",
    "Class for creating and training the neural network with 1 hidden layer to solve the Binary Classification problem on the MNIST dataset. The constructor method (_init_) loads the training and the test dataset, splits the training set into training (48000) + validation set(12000). The validation set is later used to tune the hyperparameters like the learning rate, the number of neurons in the hidden layer, the activation function for the hidden layer etc. It also initializes the weights and biases for the two layers small random values of the required dimensions using the numpy method random.random(). The random values (between 0 and 1) are multiplied by 0.01 to make them small. The number of neurons and the activation function for the hidden layer of the network is also loaded in the constructor method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "The method forwardProp() performs the forward propagation through one layer of the neural network. The parameters provided to the method are : the weights and the biases associated with the layer, input to the layer and the activation function of the layer. It returns the output form the layer. \n",
    "\n",
    "### Gradient Descent\n",
    "The method GradDescent() implements the gradient descent algorithm for learning the weights and the biases for the layers of the Neural Network. After testing various values for epochs using the validation set, I decided with 1000 iterations for the gradient descent. While increasing the number of epochs decreased the training error (not by much, though), it resulted in an increased value of the validation error.\n",
    "The method takes as input the learning rate and a boolean value for plotting the learning curve, and returns the learned values of the weights and biases for all layers.\n",
    "\n",
    "### Train Model\n",
    "The method train_model() performs the gradeint descent algorithm for the training dataset, performs the forward propagation with the learned parameters to generate the predicted values for training set and computes the cost and accuracy achieved by the trained model on the training data. If multiple learning rates are provided to the object, then train_model() performs training with each learning rate, computes the cost of the learned model on the validation set and selects the value of alpha which performs best on the validation set for the final training of the model. The accuracy is computed by rounding the predicted values upto 1 decimal place. Thus, only predicted values greater than 0.9 are rounded to 1 and less than 0.1 are rounded to 0, thus giving much better assessment of the model than just using the np.around() method. These rounded prediction values are compared with ground truth values to comput the accuracy on the training data.\n",
    "\n",
    "### Test Model\n",
    "The method test_model assess the trained model on the test data set. It performs the forward propagation with the learned parameters to generate the predicted values for the test set and computes the cost and accuracy achieved by the trained model on the test dataset. The accuracy is computed by rounding the predicted values upto 1 decimal place. Thus, only predicted values greater than 0.9 are rounded to 1 and less than 0.1 are rounded to 0, thus giving much better assessment of the model than just using the np.around() method. These rounded prediction values are compared with ground truth values to comput the accuracy on the test data.\n",
    "\n",
    "### Test Image\n",
    "The method test_image() assess the trained model on a single test image with index of the image in the test dataset provided as parameter. The function displays the ground truth value and the actual value predicted by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, x_train, y_train, x_test, y_test, alpha, neurons = 5, activation = 'ReLU'):\n",
    "        self.x_train = x_train[:, :48000]\n",
    "        self.y_train = y_train[:, :48000]\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.alpha = alpha\n",
    "        self.x_val = x_train[:, 48000:]\n",
    "        self.y_val = y_train[:, 48000:]\n",
    "        self.m = self.x_train.shape[1]\n",
    "        self.neurons = neurons\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Initializing the weights and biases using np.random.random() and multiplying them by 0.01 to initialize small values.\n",
    "        self.w1 = np.random.random((self.neurons, x_train.shape[0])) * 0.01\n",
    "        self.b1 = np.random.random((self.neurons, 1)) * 0.01\n",
    "        self.w2 = np.random.random((1, self.neurons)) * 0.01\n",
    "        self.b2 = np.random.random() * 0.01\n",
    "        \n",
    "        \n",
    "    def forwardProp(self, w, x, b, act = 'ReLU'):\n",
    "        # performs the forward propagation through one layer of the neural network\n",
    "        Z = np.dot(w, x) + b\n",
    "        if act == 'ReLU':\n",
    "            A = act_relu(Z)\n",
    "        elif act == 'tanh':\n",
    "            A = act_tanh(Z)\n",
    "        elif act == 'sigmoid':\n",
    "            A = act_sigmoid(Z)\n",
    "        return Z, A\n",
    "    \n",
    "    def GradDescent(self, a, pl = False):\n",
    "        w1 = np.copy(self.w1)\n",
    "        w2 = np.copy(self.w2)\n",
    "        b1 = np.copy(self.b1)\n",
    "        b2 = np.copy(self.b2)\n",
    "\n",
    "        for i in range(1000):\n",
    "\n",
    "            Z1, A1 = self.forwardProp(w1, self.x_train, b1, self.activation)\n",
    "            Z2, A2 = self.forwardProp(w2, A1, b2, 'sigmoid')\n",
    "            cost = - np.sum(self.y_train*np.log(A2) + (1 - self.y_train)*np.log(1 - A2))/self.m\n",
    "            dZ2 = A2 - self.y_train\n",
    "            dw2 = np.dot(dZ2, A1.T)/self.m\n",
    "            db2 = np.sum(dZ2)/self.m\n",
    "\n",
    "            dZ1 = np.dot(w2.T, dZ2)*act_gradient(A1, self.activation)\n",
    "            dw1 = np.dot(dZ1, self.x_train.T)/self.m\n",
    "            db1 = np.sum(dZ1)/self.m\n",
    "            if pl:\n",
    "                plt.plot(i, cost, 'bo')\n",
    "                plt.title('Learning Curve')\n",
    "                plt.xlabel('Iterations')\n",
    "                plt.ylabel('Cost')\n",
    "            w1 -= a*dw1\n",
    "            w2 -= a*dw2\n",
    "            b1 -= a*db1\n",
    "            b2 -= a*db2\n",
    "        return w1, b1, w2, b2\n",
    "    \n",
    "    def train_model(self):\n",
    "        # Checking to see if a single value of alpha is provided or if multiple values are provided. \n",
    "        # If multiple values are provided then the best learning rate is selected by cross validation\n",
    "        if len(self.alpha)>1:\n",
    "            # To hold the cost of models learned by various alpha values\n",
    "            cost_val = []\n",
    "            # To hold weights and biases for different learning rates\n",
    "            parameters = []\n",
    "            print('Performing Validation to tune Hyper-parameters')\n",
    "            # Looping over all the learning rates to determine the best one\n",
    "            for a in self.alpha:\n",
    "                # Performing gradient descent to update the weights and bias of the model. \n",
    "                w1, b1, w2, b2 = self.GradDescent(a)\n",
    "                # Performing forward propagation\n",
    "                Z1_t, A1_t = self.forwardProp(w1, self.x_train, b1, self.activation)\n",
    "                Z2_t, A2_t = self.forwardProp(w2, A1_t, b2, 'sigmoid')\n",
    "                \n",
    "                # Computing the vectorized cost function\n",
    "                cost = - np.sum(self.y_train*np.log(A2_t) + (1 - self.y_train)*np.log(1 - A2_t))/self.m\n",
    "                print('For alpha = ', a, 'cost on training = ', cost)\n",
    "                \n",
    "                # Forward propagation for validation dataset\n",
    "                Z1_v, A1_v = self.forwardProp(w1, self.x_val, b1, self.activation)\n",
    "                Z2_v, A2_v = self.forwardProp(w2, A1_v, b2, 'sigmoid')\n",
    "                # Computing cost over the validation set\n",
    "                cost_v = - np.sum(self.y_val*np.log(A2_v) + (1 - self.y_val)*np.log(1 - A2_v))/self.x_val.shape[1]\n",
    "                print('For alpha = ', a, 'cost on validation=', cost_v)\n",
    "                # Appending the cost to the list containing cost for different alpha values on the validation set\n",
    "                cost_val.append(cost_v)\n",
    "                # Appending the parameters for different alpha values\n",
    "                parameters.append((w1, b1, w2, b2))\n",
    "            # Selecting the alpha value with the minimum cost on the validation set\n",
    "            alpha = self.alpha[np.argmin(cost_val)]\n",
    "            print('Best value of alpha = ', alpha)\n",
    "            \n",
    "#             w1, b1, w2, b2 = parameters[np.argmin(cost_val)]            \n",
    "#             Z1_t, A1_t = self.forwardProp(w1, self.x_train, b1, self.activation)\n",
    "#             Z2_t, A2_t = self.forwardProp(w2, A1_t, b2, 'sigmoid')\n",
    "#             cost = - np.sum(self.y_train*np.log(A2_t) + (1 - self.y_train)*np.log(1 - A2_t))/self.m\n",
    "#             print('The training error = ', cost)\n",
    "            \n",
    "        else:\n",
    "            alpha = self.alpha[0]\n",
    "        print('Performing the final training')\n",
    "        # Training the model using best alpha value\n",
    "        fw1, fb1, fw2, fb2 = self.GradDescent(alpha, pl = True)\n",
    "        Z_train1, A_train1 = self.forwardProp(fw1, self.x_train, fb1, self.activation)\n",
    "        Z_train2, A_train2 = self.forwardProp(fw2, A_train1, fb2, 'sigmoid')\n",
    "        # Rounding the predicted value upto 1 decimal place\n",
    "        A_2t = np.around(A_train2, 1)\n",
    "        # Computing the accuracy over the training\n",
    "        accuracy = np.mean(A_2t == self.y_train)\n",
    "        print('Training accuracy = ', accuracy)\n",
    "        # Computing the training cost\n",
    "        cost_train = - np.sum(self.y_train * np.log(A_train2) + (1 - self.y_train)*np.log(1 - A_train2))/self.m\n",
    "        print('Final Training cost = ', cost_train)\n",
    "\n",
    "        # Saving the learned weights\n",
    "        self.w1 = fw1\n",
    "        self.b1 = fb1\n",
    "        self.w2 = fw2\n",
    "        self.b2 = fb2\n",
    "    \n",
    "    def test_model(self):\n",
    "        # Forward propagation on the test data\n",
    "        Z1_test, A1_test = self.forwardProp(self.w1, self.x_test, self.b1, self.activation)\n",
    "        Z2_test, A2_test = self.forwardProp(self.w2, A1_test, self.b2, 'sigmoid')\n",
    "        # Computing the cost on test set\n",
    "        cost_test = - np.sum(self.y_test*np.log(A2_test) + (1 - self.y_test)*np.log(1 - A2_test))/self.x_test.shape[1]\n",
    "        print('Testing Cost = ', cost_test)\n",
    "        # Computing the accuracy on test data\n",
    "        A2 = np.around(A2_test, 1)\n",
    "        accuracy = np.mean(A2 == self.y_test)\n",
    "        print('Test accuracy = ', accuracy)\n",
    "    \n",
    "    def test_image(self, i):\n",
    "        # Performing prediction on ith image in the test set\n",
    "        x = self.x_test[:, i]\n",
    "        # Reshaping to column vector\n",
    "        x = np.reshape(x, (-1, 1))\n",
    "        # getting the ground truth label\n",
    "        y = self.y_test[0, i]\n",
    "        # Forward propagation\n",
    "        Z1, A1 = self.forwardProp(self.w1, x, self.b1, self.activation)\n",
    "        Z2, A2 = self.forwardProp(self.w2, A1, self.b2, 'sigmoid')\n",
    "        # Print the results and displaying the image\n",
    "        print('Ground truth for test example {} = {}'.format(i, y))\n",
    "        print('Predicted Output for test example {} = {}'.format(i, A2))\n",
    "        plt.imshow(np.reshape(x, (28, 28)), 'gray')\n",
    "        plt.title('Test Example {}'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "Creating the object of the class Neural Network. \n",
    "After evaluating the model on the vallidation set with different values of the hyperparameters, I found the following values to give the best performance:\n",
    "learning rate - 1.1. After comparing with values including 2, 1.5, 1.2, 1.1, 1, 0.9, 0.5, 0.2, 0.1. \n",
    "Activation function for hidden layer - tanh. Compared with ReLU, tanh resulted in low cost values in most scenarios with validation set.\n",
    "Number of Neurons per hidden layer - 8. This number provided the best performance on the validation set\n",
    "\n",
    "The training cost achieved = 0.012081 (this can vary a little with each execution)\n",
    "The training accuracy achieved = 97.49375% (only predicted values greater than 0.9 are rounded to 1 and less than 0.1 are rounded to 0)\n",
    "The test cost achieved = 0.015566329\n",
    "The test accuracy achieved = 97.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mode\n",
      "Performing the final training\n",
      "Training accuracy =  0.9746666666666667\n",
      "Final Training cost =  0.012304347485987674\n",
      "Testing Mode\n",
      "Testing Cost =  0.015875617596939905\n",
      "Test accuracy =  0.9751\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# alpha = [1.2, 1.1, 1, 0.5]\n",
    "# Setting the learning rate and the activation function for the hidden layer\n",
    "alpha = [1.1]\n",
    "activation = 'tanh'\n",
    "# neurons = [5, 6, 7, 8, 9, 10]\n",
    "# for i in neurons:\n",
    "#     print('No. of neurons = ', i)\n",
    "#     LR = LogisticRegression(x_train, y_train, x_test, y_test, alpha, neurons = i)\n",
    "#     print('Training Mode')\n",
    "#     LR.train_model()\n",
    "\n",
    "# Setting the number of neurons in the hidden layer \n",
    "neurons = 8\n",
    "# Creating instance of the neural network class\n",
    "LR = NeuralNetwork(x_train, y_train, x_test, y_test, alpha, neurons = neurons, activation = activation)\n",
    "print('Training Mode')\n",
    "LR.train_model()\n",
    "print('Testing Mode')\n",
    "LR.test_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model on single test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth for test example 78 = 0\n",
      "Predicted Output for test example 78 = [[1.59040588e-05]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "LR.test_image(78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth for test example 3 = 1\n",
      "Predicted Output for test example 3 = [[0.99033583]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "LR.test_image(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
